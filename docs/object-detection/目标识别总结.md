# 排版问题，另请参考：[https://shimo.im/docs/OLRiVsaVergNfT3x/](https://shimo.im/docs/OLRiVsaVergNfT3x/) 《目标识别总结》，可复制链接后用石墨文档 App 打开
# 一、关于目标识别
## 1.1 什么是目标识别
![图片](https://images-cdn.shimo.im/ES2vdOQbUBUqcgKm/image.png!thumbnail)
（图 1）图像分类、目标识别和实例分割
在计算机视觉领域中，我们将图像分类、目标检测和实例分割称之为计算机视觉的三大基本任务(如图1)，其中：
* 图像分类：将图像划分为单个类别，通常对应于图像中最突出的物体；
* 目标识别：识别一张图像中的多个物体，并可以定位出物体(给出边界框和类别)；
* 实例分割：从图像中用目标检测的方法框出不同的实例，在不同实例区域内进行逐像素的标记。

目标检测技术是计算机视觉诸多应用的一个基本组件，实现(实时地)对不同类别物体又快又准的框定。目标检测模型的主要性能指标是检测准确度和速度，对于准确度，目标检测要考虑物体的定位准确性，而不单单是分类准确度(准表现在两个方面，定位准和分类准)。一般情况下，two-stage算法在准确度上有优势，而one-stage算法在速度上有优势。不过，随着研究的发展，两类算法都在两个方面做改进。
## 1.2 目标识别的方法
目前主流的目标检测算法主要是基于深度学习模型：
* two-stage检测算法：这类算法的典型代表是基于region proposal的R-CNN系算法，如R-CNN，Fast R-CNN，Faster R-CNN和Mask R-CNN等。原理是将检测问题划分为两个阶段，首先产生候选区域（region proposals），然后对候选区域分类（一般还需要对位置精修）。
* one-stage检测算法：比较典型的算法如YOLO、SSD和RetinaNet。主要思路是均匀地在图片的不同位置进行密集抽样，抽样时可以采用不同尺度和长宽比，然后利用CNN提取特征后直接进行分类与回归，整个过程只需要一步。

目标检测本质上面认为是在同时做分类跟定位两个任务。在传统方法时代，往往是把两个任务割裂开来，先做定位（枚举所有候选框），然后做分类。从深度学习时代，分类跟定位被耦合在网络中，彼此互相影响。深度神经网络的这几年的发展非常迅速，但是目前的网络设计对于分类跟定位这两个任务很难得到鱼和熊掌兼得。
传统的目标检测技术在接受深度学习洗礼之前多是基于 sliding window 来做的，遍历一张图片上的所有 window，据其提取 hand-crafted 特征，其 pipeline 主要是“特征+分类器”；两者最大的一个区别是基于深度学习的现代目标检测技术特征是基于深度神经网络，传统的目标检测技术需要手动设计特征，比如 Haar、HOG、LBP、ACF 等；后面再加上一个分类器，分类器主要有 SVM、Boosting 和随机森林。
深度学习出来之后，改变了整套框架。现代目标检测技术的 pipeline 为输入（图像）-> Backone（主干网络） -> Head（出口） -> NMS（后处理），可以说目前所有的深度学习检测器都是基于这套框架来做的；RetinaNet 和 Mask R-CNN 是 2017 年出现的两个非常有代表性的成果, 两者分别是 one-stage 和 two-stage 的，共同奠定了目标检测框架的基调。
# 二、目标识别方法总结
## 2.1、Sliding-window detector
滑动窗口方法是一种简单粗暴的方法，其将滑动窗口从左到右，从上到下进行滑动，使用分类模型来识别目标。为了区分不同视距下的目标，滑动窗口使用不同的尺寸和宽高比。
![图片](https://images-cdn.shimo.im/Kb3fbhH5vnUKv7b3/image.png!thumbnail)
(图 2) 滑动窗口用于生成识别目标
其操作大致可以描述为：按照滑动窗口从图片中剪切出部分图像块，因为分类模型大多需要固定的尺寸，所以会将这些图像块进行resize操作，将resize之后的图像块送入CNN分类器中提取特征(或者更原始一些hand-crafted提取特征)，利用一个SVM分类器进行分类，用一个线性回归器得到边界框。
![图片](https://images-cdn.shimo.im/VucmhQHZKccQCevw/image.png!thumbnail)
(图 3)滑动窗口用于目标检测的操作流程图
以下是伪代码: 
```
for window in windows:
  patch = get_patch(image, window)
  results = detector(patch)
```
我们生成了很多窗口来检测不同位置、不同形状的目标。 为了提高性能，减少窗口数量是一个显而易见的解决方案。
### 2.1.1 选择性搜索，Selective Search
不再用简单粗暴的方法，我们用区域提议方法（region proposal method）生成感兴趣区域(regins of interest, ROIs)来进行目标检测。在选择性搜索算法（Selective Search, SS）中，我们让每个独立像素作为一个起始的组。然后，计算每个组的纹理，合并最接近的两个组。为了避免一个区域吞并所有的其它区域，我们优先合并较小的组。持续进行合并，直到所有可能合并的区域均完成合并。图4中，第一行展示了如何进行区域生长。第二行展示了在合并过程中所有可能的ROIs。
![图片](https://images-cdn.shimo.im/wj621p81s1cuy5HA/image.png!thumbnail)
(图 4)选择性搜索
## 2.2 R-CNN
R-CNN利用区域提议方法（region proposal method）生成了约2000个感兴趣区域（regins of interest, ROIs）。这些图像块进行形变到固定的大小，分别送入到一个CNN网络中。然后，经过全连接层，进行目标分类和边界框提取。
![图片](https://images-cdn.shimo.im/RLPtRoSCntYQzhF4/image.png!thumbnail)
(图 5)R-CNN用于目标检测

![图片](https://images-cdn.shimo.im/TwurCvb2RmIIBX39/image.png!thumbnail)
(图6)R-CNN用于目标检测的流程图
很明显，R-CNN的目的是将数量更少，质量更优的识别目标送入CNN分类器中，这样R-CNN比滑动窗口的方法更快、更准确。其伪代码如下：
```
ROIs = region_proposal(image)
for ROI in ROIs:
  patch = get_patch(image, ROI)
  results = detector(patch)
```
## 2.3 Fast R-CNN
R-CNN需要足够多的提议区域才能保证准确度， 而很多区域是相互重叠的。重复地在所有的提议区域中抽取特征使得R-CNN的训练和推理过程都很缓慢。例如，我们生成了2000个的区域提议，每个提议区域分别进入CNN。换句话说，我们对不同的ROIs重复了进行了2000次的提取特征。其实，CNN中的特征映射表达了一个更紧密的空间中的空间特征，我们可以利用这些特征映射进行目标检测，而不是利用原始图像，从而催生出了Fast R-CNN。
我们不再为每个图像块重新提取特征，而是在开始时采用一个特征提取器（一个CNN网络）为整个图像提取特征。然后，直接在特征映射上应用区域提议方法。例如，Fast R-CNN选择VGG16的卷积层conv5来生成待合并ROIs来进行目标检测，其中，包括了与相应特征的映射。我们利用ROI Pooling对图像块进行形变转换成固定大小，然后将其输入到全连接层进行分类和定位（检测出目标的位置）。由于不重复特征提取，Fast R-CNN显著的缩短了处理时间。
![图片](https://images-cdn.shimo.im/Jbi5IrcurbgaqCvV/image.png!thumbnail)
(图 7)Fast R-CNN用于目标检测

![图片](https://images-cdn.shimo.im/J1omPQMb8wgGnGqK/image.png!thumbnail)
(图 8)Fast R-CNN用于目标检测的流程图
伪代码为：
```
feature_maps = process(image)
ROIs = region_proposal(feature_maps)
for ROI in ROIs:
  patch = roi_pooling(feature_maps, ROI)
  results = detector(patch) 
```
计算量很大的特征提取操作被移出了for循环。由于同时为2000个ROIs提取特征，速度有显著的提升。Fast R-CNN比R-CNN的训练速度快10倍，推理速度快150倍。Fast R-CNN的一个主要特点是整个网络（特征提取器，分类器和边界框回归器）可以通过多任务损失multi-task losses（分类损失和定位损失）进行端到端的训练，这样的设计提高了准确性。
### 2.3.1 ROI Pooling
参考文章：[https://deepsense.ai/region-of-interest-pooling-explained/](https://deepsense.ai/region-of-interest-pooling-explained/)
由于Fast R-CNN在最后使用的是全连接层，因此才会利用ROI Pooling方法将不同大小的ROIs转换为预定义尺寸的输入并将其送入全连接层。例如：
![图片](https://images-cdn.shimo.im/D4uSsykHp1Ee7f5W/image.png!thumbnail)
上图中左上角是一个特征映射图，右上角中蓝色的ROI与特征映射图重叠。为了使ROI生成目标尺寸，例如 2 x 2 ，于是将ROI分成4个大小相似或者相等的部分，每个部分取最大值，结果得到 2 x 2 的特征块，最后可以将它送入分类器和边界回归器中处理。 
![图片](https://github.com/deepsense-ai/roi-pooling/raw/master/roi_pooling_animation.gif)
## 2.4 Faster R-CNN (**)
Fast R-CNN采用类似选择性搜索(Selective Search)这样额外的区域提议方法。 但是，这些算法在CPU上运行，且速度很慢。测试时，Fast R-CNN需要2.3秒进行预测，而其中2秒花费在生成2000个ROIs上。
```
feature_maps = process(image)
ROIs = region_proposal(feature_maps)    # Expensive
for ROI in ROIs:
  patch = roi_pooling(feature_maps, ROI)
  results = detector(patch) 
```
Faster R-CNN采用与Fast R-CNN相似的设计，不同之处在于它通过内部深度网络取代区域提议方法。 新的区域提议网络（Region Proposal Network, RPN）效率更高。单幅图像生成ROIs只需要10ms。
![图片](https://images-cdn.shimo.im/i9EuM0LOUI8Ux3km/image.png!thumbnail)
![图片](https://images-cdn.shimo.im/RZlLM9eDDk02dEWg/image.png!thumbnail)
![图片](https://images-cdn.shimo.im/zdWsqn5DMKIQI32U/image.png!thumbnail)
![图片](https://images-cdn.shimo.im/r38h6ddrvtUmcwg9/image.png!thumbnail)
### 2.4.1 区域提议网络(Region proposal network)
区域提议网络(RPN)用第一个卷积网络输出的特征图作为输入。在特征图上用3×3的滤波器进行滑动（滤波），采用诸如ZF网络（如下图）的卷积网络来得到未知类的建议区域。其他如VGG或者ResNet可以被用来提取更全面的特征，但需以速度为代价。ZF网络输出的256个值分别被送入两个不一样的全连接层来预测边界框和对象性分数（2 objectness score)。对象性描述了框内是否包含有一个物体。我们可以用回归器来计算单个物体的分数，但是为了简单起见，Faster R-CNN使用了一个分类器分类出两种可能的类别：“存在物体”类和“不存在物体/背景”类。
RPN对特征图里的每个位置(像素点)做了K次猜测。因此RPN在每个位置都输出4×k个坐标和2×k个分数。以下图例演示了一个使用3*3过滤器的8*8特征图，它一共输出8×8×3个兴趣区(ROI)(当k=3时）。右侧图例展示了在单个位置得到的3个提议区域。
![图片](https://images-cdn.shimo.im/Fu0FpdfTcr8RXYsD/image.png!thumbnail)
我们现在有3个猜测，随后我们也会逐渐改善我们的猜想。因为我们最终只需要一个正确的猜测，所以我们使用不同形状和大小的的初始猜测会更好。因此，Faster R-CNN不是随机的选择提议边界框，而是预测了相对于一些被称为锚的参考框的左上角的偏移量，比如 delta x, delta y 。因为我们约束了偏移量，所以我们的猜测仍然类似于锚。
![图片](https://images-cdn.shimo.im/OGRS4KobXdgGcNfp/image.png!thumbnail)
为了对每个位置都进行k次预测，我们需要在每个位置中心放置k个锚。每次预测都和不同位置但是相同形状的特定锚相关。
![图片](https://images-cdn.shimo.im/2yzr7xwjpdQdAuYg/image.png!thumbnail)
这些锚都是精心预选好的，所以它们多种多样，同时非常合理的覆盖了不同尺度和不同长宽比的现实生活中的物体，这使得初始训练将具有更好的猜测，同时允许每次预测都有特定、不同的形状。这种方式使早期的训练更加稳定和容易。
Faster R-CNN使用了更多的锚。Faster R-CNN在一个位置上使用了9个锚: 3种不同尺度并使用三种长宽比。在每个位置使用9种锚，所以对于每个位置，它一共产生了2*9个对象性分数和4×9个坐标。锚在不同的论文中也被称为先验或者默认边界框。
![图片](https://images-cdn.shimo.im/2VPZqvxiXqMOzOF5/image.png!thumbnail)
伪代码：
```
feature_maps = process(image)
ROIs = RPN(feature_maps) 
for ROI in ROIs:
  patch = roi_pooling(feature_maps, ROI)
  class_scores, box = detector(patch)
  class_probabilities = softmax(class_scores)
```
### ![图片](https://images-cdn.shimo.im/hfqER2Hm0AcvbQyK/image.png!thumbnail)
### 2.4.2 R-CNN系列的性能比较
![图片](https://images-cdn.shimo.im/ysXo6AEOBwc0gYzU/image.png!thumbnail)
## 2.5 R-FCN
假设我们只有一张特征图用来检测脸上的右眼。我们是否可以用此来决定脸的位置呢？是可以的。因为右眼应该位于一张面部图像的左上角，我们也可以用此信息来确定脸的位置。
![图片](https://images-cdn.shimo.im/3gzvolGN3RUnBPno/image.png!thumbnail)
如果我们有另外的特征图专门用来分别检测左眼，鼻子，嘴，我们可以将这些结果结合在一起使对脸部的定位更准确。那为什么我们要如此麻烦呢？在Faster R-CNN里，检测器使用多个全连接层来做预测，有2000多个ROI，这消耗很高。
```
feature_maps = process(image)
ROIs = RPN(feature_maps) 
for ROI in ROIs:
  patch = roi_pooling(feature_maps, ROI)
  class_scores, box = detector(patch)    # Expensive
  class_probabilities = softmax(class_scores)
```
R-FCN通过减少每个ROI需要的工作总量来提高速度，以上基于区域的特征图独立于ROIs，同时可以在每一个ROI的外部进行计算。接下来的工作就更简单了，因此R-FCN比Faster R-CNN要快。
```
feature_maps = process(image)
ROIs = region_proposal(feature_maps) 
score_maps = compute_score_map(feature_maps)
for ROI in ROIs:
  V = region_roi_pool(score_maps, ROI)
  class_scores, box = average(V)    # Much simpler
  class_probabilities = softmax(class_scores)
```
我们可以想想一下这种情况，M是一个5*5大小，有一个蓝色的正方形物体在其中的特征图，我们将方形物体平均分割成3*3的区域。现在我们从M中创建一个新的特征图并只用其来检测方形区域的左上角。这个新的特征图如下右图，只有黄色网格单元被激活。
![图片](https://images-cdn.shimo.im/5uR1vEBYToEbqQLB/image.png!thumbnail)
因为我们将方形分为了9个部分，我们可以创建9张特征图分别来检测对应的物体区域。因为每张图检测的是目标物体的子区域，所以这些特征图被称为位置敏感分数图（position-sensitive score maps）。
![图片](https://images-cdn.shimo.im/sJ23ZxiGPocZw0Go/image.png!thumbnail)
比如，我们可以说，下图由虚线所画的红色矩形是被提议的ROIs。我们将其分为3*3区域并得出每个区域可能包含其对应的物体部分的可能性。例如，ROIs的左上区域中存在左眼的可能性。我们将此结果储存在3*3的投票阵列（如下右图）中。比如，投票阵列[0][0]中数值的意义是在此找到方形目标左上区域的可能性。
![图片](https://images-cdn.shimo.im/wKqZxUguUjsfek2i/image.png!thumbnail)
将分数图和ROIs映射到投票阵列的过程叫做位置敏感ROI池化（position-sensitive ROI-pool）。这个过程和我们之前提到的ROI pool非常相似。这里不会更深入的去讲解它，但是你可以参考以后的章节来获取更多信息。
![图片](https://images-cdn.shimo.im/F7lC4Izi1wETT1vW/image.png!thumbnail)
在计算完位置敏感ROI池化所有的值之后，分类的得分就是所有它元素的平均值。
![图片](https://images-cdn.shimo.im/tsPFyGBEqxI9IIvE/image.png!thumbnail)
如果说我们有C类物体需要检测。我们将使用C+1个类，因为其中多包括了一个背景（无目标物体）类。每类都分别有一个3×3分数图，因此一共有(C+1)×3×3张分数图。通过使用自己类别的那组分数图，我们可以预测出每一类的分数。然后我们使用softmax来操作这些分数从而计算出每一类的概率。
接下来是数据流（图），比如我们的例子中，k=3。
![图片](https://images-cdn.shimo.im/l9WWZYuIjXgbwodX/image.png!thumbnail)
## 2.6 YOLO
## 2.7 SSD (**)
## 2.8 FPN
![图片](https://images-cdn.shimo.im/jXKcX4QhgqoPOcG9/image.png!thumbnail)
上图主要对比了针对多尺度问题的各种解决思路：
* 由原图生成图像金字塔，然后由这些图像生成相应的特征图并进行预测；
* 在原图上进行卷积，在最后一层进行预测；
* 使用多个卷积特征图进行预测，SSD就是这么干的，但是SSD使用的卷积特征图只是后来加入的网络层，前面的卷积层没有使用，而这些卷积特征图对于检测小的目标至关重要；
* FPN利用各层的卷积特征图，并逐步微调。



## 2.9 RetinaNet (**)
### 2.9.1 关于样本更细的分类
![图片](https://images-cdn.shimo.im/ojewma4fOhsJS0Qp/image.png!thumbnail)
### 2.9.2 Abstract
目前，目标识别准确度最高的detector是形如R-CNN的的two-stage方法，其分类器作用于一系列稀疏的条件对象区域。与之相比，one-stage detector作用于一些列规则的并且密集的对象区域。one-stage 方法更简单更快，但是准确度却没有two-stage方法那么高。
one-stage detector精度不高的原因在于extreme forground-background class imbalance，这种不平衡导致了训练的时候存在大量的easy examples（包括easy positive和easy negative，但主要是easy negative），这些easy example虽然每一个的loss会比较小，但是因为数量巨大，主导了最终的loss，导致最后训练出来的是一个degenerated model。
为此，作者提出了一个dynamically scaled cross entropy loss，即focal loss来替换原来detector classification部分的standard cross entropy loss。通过降低训练过程中easy example的权重，提高了模型的训练效果，使得one-stage detector在精度上能够达到乃至超过two-stage detector。
### 2.9.3 Focal Loss
原来的分类loss是各个训练样本交叉熵的直接求和，也就是各个样本的权重是一样的。如下式所示：
![图片](http://ofdz11dcz.bkt.clouddn.com/FL_1.png)
其中，CE表示cross entropy，p表示预测样本属于1的概率，y表示label，y的取值为{+1,-1}，这里仅仅以二分类为例，多分类分类以此类推。 
为了表示简便，我们用p_t表示样本属于true class的概率。所以上述(1)式可以写成
![图片](http://ofdz11dcz.bkt.clouddn.com/FL_2.png) 
样本概率对损失函数的关系用下图表示：
![图片](http://ofdz11dcz.bkt.clouddn.com/FL_3.png)
从蓝线我们可以看出来，对于well-classified样本(true class的预测概率大于0.6为例），他们的loss仍然不算小。对于yolo/ssd这类one-stage detector，他们没有rpn这类proposal网络，而是对图像上densely sampled window进行检测，这样的话负样本的数量会远远超过正样本的数量，而负样本里面绝大多数又是easy example，也就是说，对于one-stage detector来说，虽然单个easy example的loss不大，但是所有easy example的loss加起来就会远远超过hard example的loss了。当hard example的loss对于模型的更新没有什么影响的时候，得到的分类模型的分类边界往往不够准确，对于hard example比较容易分错。
既然one-stage detector在训练的时候正负样本的数量差距很大，那么一种常见的做法就是给正负样本加上权重，负样本出现的频次多，那么就降低负样本的权重，正样本数量少，就相对提高正样本的权重，如下式所示： 
![图片](https://images-cdn.shimo.im/HxnL3T7DAf0HetGn/image.png!thumbnail)
但是问题的实质是easy example过多（包括easy positive和easy negative，但主要是easy negative），那么实际上要解决的是easy example和hard example不均衡的问题，这个和训练时候正负样本不均衡是两码事，因为正负样本里面都会有简单的样本和容易分错的样本。
focal loss，相当于是对各个样本加上了各自的权重，这个权重是和网络预测该样本属于true class的概率相关的，显然，如果网络预测的该样本属于true class的概率很大，那么这个样本对网络来说就属于easy(well-classified) example。如果网络预测的该样本属于true class的概率很小，那么这个样本对网络来说就属于hard example。为了训练一个有效的classification part，显然应该降低绝大部分easy example的权重，相对增大hard example的权重。focal loss如下式所示： 
![图片](https://images-cdn.shimo.im/fHcGnhN8wgIxOyqL/image.png!thumbnail)
参数gamma大于0，当gamma=0的时候，就是普通的交叉熵，实验中发现当gamma=2的效果最好。可以指出，当gamma=2，一样的easy example(p_t =0.9)的loss要比标准的交叉熵loss小100+倍，当p_t=0.968，小1000+倍，然而，对于hard example(p_t < 0.5)，loss最多小了4倍。这样的话，hard example的权重相对就提升了很多。为了兼顾正负样本均衡，得到：
![图片](https://images-cdn.shimo.im/721BTKgeDgwNNdtX/image.png!thumbnail)
### 2.9.4 RetinaNet Detector








### 2.10 Mask R-CNN
# 三、基于深度学习的目标检测面临的挑战
the devil is in the detail，从五个细节点着手去填补理想与现实这一差距，通过从这些细节点出发，不断优化这项技术，才能推动其走向产品化，更实用、更落地。
现代目标检测的基本框架：输入（图像）-> Backone（主干网络） -> Head（出口） -> NMS（后处理），这其中暴漏了五个潜在的难题：
* Backbone，目前主要使用的是 ImageNet Pretrained models，比如 VGG-16、ResNet、DenseNet 等；
* Head，传统的 two-stage 网络慢的主要原因在于 head 太厚，可以对其进行加速，变得更薄、更快；
* Scale，比如图像中人像的尺寸变化很大，大到 2k x 2k，小到 10 x 10，如何使神经网络算法更好地处理物体尺寸的变化是一个非常有趣的话题；
* Batch Size，传统目标检测器的 batch size 非常小，为什么不尝试使用更大的 batch size 呢？
* Crowd，无论是传统还是现代目标检测器，都会使用到一个叫做 NMS 后处理步骤，但是目前现有的 Benchmark 对 NMS 的优化并不敏感。

虽然 RetinaNet 和 Mask R-CNN 出来之后成绩很好，但是一些细节之处依然有待提升。

